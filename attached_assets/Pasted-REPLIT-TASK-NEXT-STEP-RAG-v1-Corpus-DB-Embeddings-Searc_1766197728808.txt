REPLIT TASK (NEXT STEP): RAG v1 — Corpus DB + Embeddings + Search Endpoint (WRITE CODE + APPLY CHANGES)

Goal: Implement a production-grade “learning database” for HiveMind using Postgres + pgvector + Ollama embeddings. Use approved corpus only.

A) Database changes (pgvector + tables)

Enable pgvector extension: CREATE EXTENSION IF NOT EXISTS vector;

Add/extend tables:

corpus_items

id

trackId (optional)

title

content (full text)

status: draft | approved | rejected

createdByWallet

createdAt / updatedAt

approvedAt (nullable)

cycleId (nullable; later)

corpus_chunks

id

corpusItemId (fk)

chunkIndex (int)

chunkText (text)

embedding (vector(N)) ← choose correct dimension for the embedding model

embeddingModel (text)

createdAt

Indexes:

vector index on corpus_chunks.embedding (IVFFLAT/HNSW if available)

btree index on corpusItemId

B) Embedding service (Ollama)

Implement embedding generation using Ollama’s embeddings API:

Prefer POST /api/embed with { model, input } 
Ollama Documentation
+1

Pick a dedicated embedding model (don’t use your chat model), e.g. embeddinggemma or nomic-embed-text. 
Ollama Documentation
+1

Add env vars:

OLLAMA_EMBED_MODEL (default embeddinggemma)

C) Chunking

When a corpus item becomes approved:

split content into chunks (simple: ~800–1200 chars with overlap)

generate embeddings for each chunk

store in corpus_chunks

Important: only embed approved items (keeps learning clean).

D) Retrieval endpoint

Add:

POST /api/rag/search

Auth required (or token-gated if you prefer)

Body: { query, k, trackId? }

Server:

embed the query (same embed model)

vector search in corpus_chunks

return top K chunks with { corpusItemId, chunkText, score }

E) Integrate into chat (grounding)

Update POST /api/ai/chat:

Before calling LLM:

call retrieval with the user message

inject “Sources” chunks into the system prompt

Return response + the sources used (for UI display)

Rule: if 0 sources found, either:

answer with “not enough grounded data” OR

allow general answer but label it clearly as ungrounded (your choice)

F) Deliverable

Return:

Files changed

New env vars + defaults

Example response from /api/rag/search

Example chat response showing sources[]

Confirm embeddings are only generated for approved corpus

Implement now.